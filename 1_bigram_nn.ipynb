{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Neural Net Model â€“ Makemore (Karpathy)\n",
    "This notebook implements a bigram character-level language model using a single-layer neural network, in contrast to the bigram_count.ipynb that used frequencies to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Overview\n",
    "1. The NN will receive a character as input\n",
    "2. There will be some params (weights)\n",
    "3. The output will be a prob distribution for the next character\n",
    "4. The output of the NN will be evaluated using the loss function \n",
    "5. The weights will be modified using gradient based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "1. Read the names from the dataset\n",
    "2. Create a vocabulary of characters\n",
    "3. Create a training set of bigrams (x,y)\n",
    "4. Convert the training set of bigrams into one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read the names from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Load a list of names from the dataset (each name on a new line)\n",
    "words = open(\"data/names.txt\", 'r').read().splitlines()\n",
    "print(words[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a vocabulary of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary of characters from all names\n",
    "vocab = sorted(set(\"\".join(words)))\n",
    "\n",
    "# Add '.' as the start/end token, mapped to index 0\n",
    "# create a mapping from characters to integers\n",
    "stoi = {s: i+1 for i, s in enumerate(vocab)}\n",
    "stoi['.'] = 0\n",
    "# create a mapping from integers to characters\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "print(itos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a training set of bigrams (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training set of bigrams (x,y) - where x is the preceeding character and y is the following character.\n",
    "#### x will be the input, while y will be the label. \n",
    "The neural net will predict y for a given value of x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset of bigram (x, y) pairs: x is the current char, y is the next char\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chars = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        # print(f\"ch1: {ch1}, ch2: {ch2}\")\n",
    "        print(ch1, ch2)\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the training set of bigrams from list to tensors for efficient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([ 0,  5, 13, 13,  1])\n",
      "labels:tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# convert inputs and labels to tensors so we can work in pytorch\n",
    "# use torch.tensor instead of torch.Tensor as the dtype for torch.tensor is int64 and that of torch.Tensor is float32. \n",
    "# since we are just storing numbers, we can use torch.tensor\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"input: {xs}\")\n",
    "print(f\"labels:{ys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are considering only the first word \"emma\". \n",
    "\n",
    "The first word \"emma\" can be broken down into five examples. \n",
    "\n",
    ". e\n",
    "\n",
    "e m\n",
    "\n",
    "m m\n",
    "\n",
    "m a\n",
    "\n",
    "a .\n",
    "\n",
    "So the input has 5 examples.\n",
    "\n",
    "According to the above input xs and lables ys, when the input to the network is 0, the label is 5, so the predicted output should be 5, which corresponds to e, that is the probability of 5th index in the output should be the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert the training set of bigrams into one-hot encoded vectors\n",
    "\n",
    "Only the appropriate bit (index) will be 1, all the remaining indices will be 0.\n",
    "\n",
    "This will be the input to the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]])\n",
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# num_classes = 27 because the vocab size is 27\n",
    "# if we dont mention num_classes, one_hot will assume the number or classes to be 14, as the highest value in xs is 13\n",
    "# however, if we were using the entire corpus and not just the 1st word, then the highest value would have been 26 and the num_class would have\n",
    "# automatically be considered to be 27\n",
    "xenc = F.one_hot(xs, num_classes=27)\n",
    "print(xenc.shape)\n",
    "print(xenc)\n",
    "print(xenc.dtype)\n",
    "# the dtype of xenc is int, but we always give floating point numbers to a nn that can take on continuous values.\n",
    "# so convert the dtype of xenc to float\n",
    "xenc = xenc.float()\n",
    "print(xenc.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights\n",
    "\n",
    "\n",
    "#### Construct a single neuron that perform W*X + b\n",
    "Here we will only be doing W*X\n",
    "\n",
    "1. Initialize W\n",
    "2. Matrix multiplication @ between W and input vector xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of xenc: torch.Size([5, 27])\n",
      "\n",
      "Shape of Weight matrix W: torch.Size([27, 1])\n",
      "\n",
      "Shape of xenc @ W: torch.Size([5, 1])\n",
      "\n",
      "Mat mul xenc @ W: tensor([[-1.2920],\n",
      "        [-0.5032],\n",
      "        [-0.4323],\n",
      "        [-0.4323],\n",
      "        [-0.9469]])\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights W using randn - which draws random numbers from a normal distribution\n",
    "# size = (27,1) means we create 27 weights for one neuron\n",
    "# W will be a column vector of 27 numbers\n",
    "# number of characters in vocab\n",
    "vocab_size = len(itos)\n",
    "W = torch.randn(size = (vocab_size,1))\n",
    "print(f\"Shape of xenc: {xenc.shape}\")\n",
    "print(f\"\\nShape of Weight matrix W: {W.shape}\")\n",
    "# matrix multiplication of W and input\n",
    "print(f\"\\nShape of xenc @ W: {(xenc @ W).shape}\")\n",
    "print(f\"\\nMat mul xenc @ W: {xenc @ W}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input is (5,27)\n",
    "\n",
    "weights is (27, 1) : 27 = number of weights, 1 = number of neurons\n",
    "\n",
    "The output of xenc @ W will be a (5, 1) vector, which  provides us the 5 activations of a single neuron on the 5 input examples.\n",
    "We fed all the five inputs to a single neuron, and pytorch handled all of the inputs simultaneously. In micrograd, we had to use a loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 27 neurons instead of a single neuron\n",
    "\n",
    "The reason we need 27 neurons instead of a single neuron is that we need a probability distribution of 27 characters. A single neuron's output was(5,1), meaning only a single probability, but 27 neurons output will be (5,27), which means a probability distribution of 27 characters.\n",
    "\n",
    "Another reason is, that our end goal is to create a matrix equivalent to that of matrix P as we had in the simple count based bigram model. P was of size (27,27), where each row represented the first character of a biagram and each column respresented the second character of a bigram .\n",
    "The values inside each row represented the probability distribution of bigrams of a certain character. E.g, the row indexed 1, had the probability distribution of all the bigrams of character a. ab, ac, ad, ...., az, *a`.`* (*`.`* is the end token).\n",
    "\n",
    "*W with shape (27,27) means we have 27 neurons with each neuron having 27 connections (connections have the weights)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Logits (log-counts) and Counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W: torch.Size([27, 27])\n",
      "\n",
      "Shape of xenc: torch.Size([5, 27])\n",
      "\n",
      "Shape of logits: torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights W using randn - which draws random numbers from a normal distribution\n",
    "# size = (27,27) means we create 27 weights for 27 neurons\n",
    "# W will be a column vector of 27 numbers\n",
    "vocab_size = len(itos) # 27\n",
    "# initialize weights\n",
    "W = torch.randn((vocab_size, vocab_size), requires_grad=True)\n",
    "\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"\\nShape of xenc: {xenc.shape}\")\n",
    "\n",
    "# calculate logits\n",
    "y = xenc @ W\n",
    "print(f\"\\nShape of logits: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of xenc @ W will be a (5, 27) vector. \n",
    "The first row will provide us the 27 activations of the 27 neurons for thr first example.\n",
    "This is achieved as a result of dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0247, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)[3][13] # gives the activation of the 13th neuron in the 3rd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1 of logits: tensor([-0.0340,  0.1212,  0.0355, -1.1042,  1.0493,  0.2631,  1.0342, -1.4824,\n",
      "         0.7879, -0.9840, -0.0260,  0.2059,  0.1654, -1.2550, -1.3606,  0.0512,\n",
      "         0.5799, -0.4467,  2.2150,  0.7303,  0.1610, -1.3518,  1.0963,  0.7427,\n",
      "         0.1634, -1.0055, -2.0936], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"row 1 of logits: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the output of Logits (xenc @ W)\n",
    "xenc @ W is a (27,27) matrix with positive and negative numbers.\n",
    "\n",
    "The end goal is to produce the same matrix as P in the simple bigram model. P was derived by N which had the counts of every bigram.\n",
    "To achieve this goal, we want to convert the output of xenc @ W into a representation that can be considered as counts, and then we can normalize those counts to get prob distributions, just like we did in the simple bigram model.\n",
    "\n",
    "`log-counts`: So, we assume that the output of xenc @ W is the *log* of counts (log counts), as log can take both positive and negative values and the output of xenc @ W has both positive and negative values. \n",
    "\n",
    "`Exponent to get Counts`: To get the counts out of the log counts, we exponentiate the log counts.\n",
    "\n",
    "Exponent of anything negative is less than 1 and greater than zero. i.e between 0 and 1\n",
    "\n",
    "Exponents of anything positive is greater than 1.\n",
    "\n",
    "##### ```After performing element-wise exponent on xenc @ W, ```\n",
    "\n",
    "*negative numbers will be tranformed to positive numbers (0 and 1).*\n",
    "    \n",
    "*positive numbers will become even more positive*\n",
    "\n",
    "** The exponents of log counts can be interpreted as counts as these values are always positive**\n",
    "\n",
    "The output of *xenc @ W* is known as *log-counts* or *logits*\n",
    "The output of exp of logits is the *counts* which represent a matrix that is equivalent to N matrix\n",
    "\n",
    "Once we have the counts, we can get the probabilites simply by normalizing the counts, just as was done for the simple bigram model.\n",
    "Every row of prob matrix should sum to 1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two steps of taking exp to get counts and then normalizing those counts to get probability distributions are collectively the *SOFTMAX* function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([-0.0340,  0.1212,  0.0355, -1.1042,  1.0493,  0.2631,  1.0342, -1.4824,\n",
      "         0.7879, -0.9840, -0.0260,  0.2059,  0.1654, -1.2550, -1.3606,  0.0512,\n",
      "         0.5799, -0.4467,  2.2150,  0.7303,  0.1610, -1.3518,  1.0963,  0.7427,\n",
      "         0.1634, -1.0055, -2.0936], grad_fn=<SelectBackward0>)\n",
      "counts: tensor([0.9666, 1.1288, 1.0362, 0.3315, 2.8557, 1.3010, 2.8128, 0.2271, 2.1987,\n",
      "        0.3738, 0.9744, 1.2287, 1.1799, 0.2851, 0.2565, 1.0525, 1.7858, 0.6398,\n",
      "        9.1618, 2.0757, 1.1746, 0.2588, 2.9931, 2.1016, 1.1775, 0.3659, 0.1232],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "probs: tensor([0.0241, 0.0282, 0.0259, 0.0083, 0.0713, 0.0325, 0.0702, 0.0057, 0.0549,\n",
      "        0.0093, 0.0243, 0.0307, 0.0294, 0.0071, 0.0064, 0.0263, 0.0446, 0.0160,\n",
      "        0.2287, 0.0518, 0.0293, 0.0065, 0.0747, 0.0525, 0.0294, 0.0091, 0.0031],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Shape of probs:torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "logits = xenc @ W #log-counts\n",
    "counts = logits.exp() # counts, equivalent N\n",
    "probs = counts / counts.sum(1, keepdim=True) # counts, probs is the softmax\n",
    "print(f\"logits: {logits[0]}\") # poitive and negative values\n",
    "print(f\"counts: {counts[0]}\") # all positive values\n",
    "print(f\"probs: {probs[0]}\") # [[each row should sum up to 1\n",
    "print(f\"Shape of probs:{probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the probs matrix tell us\n",
    "\n",
    "Shape of the input (xenc) is (5,27)\n",
    "\n",
    "Shape of probs is (5, 27)\n",
    "\n",
    "The probs matrix gives us:\n",
    "\n",
    "For every example in the input vector, it gives us the probability distribution of the next character.\n",
    "\n",
    "Probs[0] will represent the output prob distribution of the first input example. It will have 27 values. \n",
    "\n",
    "It will tell us *HOW LIKELY IS EACH CHARACTER TO COME NEXT*.\n",
    "\n",
    "  The first value will give the probability of the start token, the next value will give the prob of the next character to be \"a\", the third will have prob for \"b\" and so on, such that all these probabilities will sum up to 1, giving us a prob distribution for the next character.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0576, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[1][1] # prob of the next character to be \"a\" for the second input which was \"e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we tune W, the values of probs will change. The ultimate goal will be to achieve the same values of probs as the matrix P(in bigram_count.ipynb), as it represented the real probabilities for the given dataset. \n",
    "\n",
    "**The way to tune W is by the LOSS FUNCTION.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "A forward pass performs all the steps from one-hot encoding the input to calculating logits\n",
    "\n",
    "1. One-hot encode input\n",
    "2. Initialize weights\n",
    "3. Logits (log-counts)\n",
    "4. Exponentiate logits (get counts)\n",
    "5. Normalize counts\n",
    "6. Negative log likelihood\n",
    "7. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input xs: tensor([ 0,  5, 13, 13,  1])\n",
      "Raw labels ys: tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw input xs: {xs}\")\n",
    "print(f\"Raw labels ys: {ys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of probs matrix: torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "# geneartor for reproducibility\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# one-hot encode input\n",
    "xenc = F.one_hot(xs,num_classes=27).float()\n",
    "# initialize weight matrix\n",
    "W = torch.randn((27,27), generator=g)\n",
    "# calculate logits\n",
    "logits = xenc @ W  #(5,27), (27,27) -> (5,27)\n",
    "# calculate counts\n",
    "counts = logits.exp()\n",
    "# calculate probability\n",
    "probs = counts / counts.sum(1, keepdim=True) # the exp and the sum step constiture the softmax function\n",
    "print(f\"Shape of probs matrix: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Probabilities and Loss Calculation**\n",
    "\n",
    "`probs` has 5 rows, each row representing the probability of 27 characters to be the next character. However, we are only interested in the probability of the character that is the actual label.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "For the first input example:\n",
    "- `xs[0] = 0` (`.`)\n",
    "- Ground truth is `ys[0] = 5` (`e`)\n",
    "\n",
    "So, for the output probability distribution for the first input example, there are 27 probabilities, but we are only interested in the probability of the 5th index, which is the probability of `e` to be the next character. We want this probability to be maximized, and the gap between the predicted probability for the 5th character and the actual probability of the 5th character (i.e., actual probability = 1) is the **loss**.\n",
    "\n",
    "**Likelihood**: The prob of the correct index tells us the **likelihood** of the model on that example. so if the prob of \"e\" is 0.0123, it means that the model is 1.2 percent accurate on that particular example. ``\n",
    "\n",
    "Adding the likelihoods of all the examples will give the likelihood of the model on the entire dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Extracting the 5th Probability**\n",
    "\n",
    "We need to extract the 5th probability from the 0th row of `probs`. This can be done in two ways:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Indexing:**\n",
    "\n",
    "For the 0th example, index the label index from the 0th row:\n",
    "\n",
    "```python\n",
    "probs[0, 5]\n",
    "\n",
    "\n",
    "0 is the index of the input, corresponding to rows of probs.\n",
    "\n",
    "5 is the index of the label, which we got from ys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Dot Product of Predicted Probabilities with Actual Probabilities of Labels**\n",
    "\n",
    "The predicted probability distribution for the first example is:\n",
    "\n",
    "```python\n",
    "probs[0] = [0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
    "            0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
    "            0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459]\n",
    "```\n",
    "\n",
    "To get the actual probability of labels, we one-hot encode the labels vector `ys`:\n",
    "\n",
    "```python\n",
    "ys = [5, 13, 13, 1, 0]\n",
    "```\n",
    "\n",
    "The one-hot encoded vector for the first label `5` becomes:\n",
    "\n",
    "```python\n",
    "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "Here, `1` is only at the 5th index, while all other indices are `0`.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Dot Product Calculation:**\n",
    "\n",
    "The dot product of `probs[0]` and the one-hot encoded vector for the first label is:\n",
    "\n",
    "```python\n",
    "[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
    " 0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
    " 0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459]\n",
    " *\n",
    "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "This results in:\n",
    "\n",
    "```python\n",
    "[0, 0, 0, 0, 0, 0.0123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "Taking the sum of this vector gives:\n",
    "\n",
    "```python\n",
    "0.0123\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate NLL and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3993, 4.0146, 3.6234, 2.6081, 4.2012])\n"
     ]
    }
   ],
   "source": [
    "# Caluculating NLL for the 5 examples of \"emma\" using indexing\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    y = ys[i].item()\n",
    "    # likelihood of the correct index\n",
    "    p = probs[i, y] # using indexing\n",
    "    # calculate log of p (loglikelihood)\n",
    "    log_p = torch.log(p)\n",
    "    # calculate neg of log_p (NLL)\n",
    "    nll = -log_p\n",
    "    nlls[i] = nll\n",
    "print(nlls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity:\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    y = ys[i].item()\n",
    "    \n",
    "    p = probs[i, y]\n",
    "\n",
    "these three lines can be replaced by \n",
    "p = probs[torch.arange(5), ys]\n",
    "\n",
    "\n",
    "So the code becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood of correct indices: tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])\n",
      "[tensor([4.3993, 4.0146, 3.6234, 2.6081, 4.2012])]\n"
     ]
    }
   ],
   "source": [
    "# pluck out the probability from prob for values in ys \n",
    "p = probs[torch.arange(5), ys] \n",
    "print(f\"likelihood of correct indices: {p}\")\n",
    "# log-likelihood\n",
    "log_p = torch.log(p)\n",
    "# negiative log-likelihood\n",
    "nll = -log_p\n",
    "nlls = []\n",
    "nlls.append(nll)\n",
    "print(nlls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3993, 4.0146, 3.6234, 2.6081, 4.2012])\n"
     ]
    }
   ],
   "source": [
    "# Caluculating NLL for the 5 examples of \"emma\" using the dot product approach \n",
    "# using the dot product is the standard approach of plucking out the prob of the right index\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    yenc = F.one_hot(ys[i], num_classes=27)\n",
    "    prob_i = probs[i]\n",
    "    p = (yenc * prob_i).sum()\n",
    "    log_p = torch.log(p)\n",
    "    nll = -log_p\n",
    "    nlls[i] = nll\n",
    "print(nlls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the nlls vector\n",
    "the nlls vector has 5 values, [4.3993, 4.0146, 3.6234, 2.6081, 4.2012]. Each value provides the negative log likelihood for an input example.\n",
    "The first value 4.3993 is the the NLL for the first input (.), the second value in the NLL for the second input (e) and so on. \n",
    "\n",
    "The NLLs for all examples are very high, which means the loss will be very high.\n",
    "\n",
    "#### Calculate Loss from NLL\n",
    "Loss is simply the mean of all the nlls across the dataset. 5 examples (one word) in our case.\n",
    "\n",
    "Changing W will change the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7693049907684326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "loss = (nlls.sum()/len(nlls)).item() # nlls.mean().item()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the Forward Pass and loss calcualtion\n",
    "1. One-hot encode input\n",
    "2. Initialize weights\n",
    "3. Logits (log-counts)\n",
    "4. Exponentiate logits (get counts)\n",
    "5. Normalize counts\n",
    "6. Negative log likelihood\n",
    "7. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "xenc = F.one_hot(xs,num_classes=27).float()\n",
    "W = torch.randn((27,27), generator=g, requires_grad= True) # set requires grad= True to make sure grads for W are stored\n",
    "logits = xenc @ W  #(5,27), (27,27) -> (5,27)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True) # predictions\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial values of a subset of W matrix\n",
      " tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
      "         0.0791,  0.9046], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"initial values of a subset of W matrix\\n {W[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation to Optimize the Weights Using Gradient Based Optimization\n",
    "\n",
    "Optimize the weights using Gradient Descent. Conside learning rate to be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W.grad = None # set the grads to zero. in pytorch, we set grad to None which is equal to setting it to 0\n",
    "loss.backward() # calculate grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss.backward() will store the grads for all the weights (params) and W.grad won't be NONE anymore\n",
    "\n",
    "Every element of W.grad is telling us the impact of that W on the loss. \n",
    "If the grad is positive, the W has a positive impact, if the loss is negative, the weight has a negative impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n"
     ]
    }
   ],
   "source": [
    "print(W.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weights using gradient descent\n",
    "lr = 0.5\n",
    "W.data += -lr * W.grad # W.data = W.data - (0.1 * W.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated values of a subset of W matrix W:\n",
      "tensor([ 1.5613, -0.2383, -0.0286, -1.1012,  0.2842,  0.0691, -1.5473,  0.6026,\n",
      "         0.0778,  0.9015], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"updated values of a subset of W matrix W:\\n{W[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in W have changed slightly after Gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing Backprop\n",
    " Set the grads to zero. in pytorch, we set grad to None which is equal to setting it to 0 (W.grad = None)\n",
    "\n",
    " Calculate grads (loss.backward())\n",
    "\n",
    " Update weights (W.data += -0.5 * W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running 10 iterations of forward and backward pass to reduce the loss\n",
    "\n",
    "The loss decreases from 3.769 to 3.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:3.7693049907684326\n",
      "loss:3.7492127418518066\n",
      "loss:3.7291626930236816\n",
      "loss:3.7091546058654785\n",
      "loss:3.6891887187957764\n",
      "loss:3.6692662239074707\n",
      "loss:3.6493873596191406\n",
      "loss:3.629552125930786\n",
      "loss:3.6097614765167236\n",
      "loss:3.5900158882141113\n"
     ]
    }
   ],
   "source": [
    "# initialize the loss function\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad= True) # set requires grad= True to make sure grads for W are stored\n",
    "\n",
    "for i in range(10):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs,num_classes=27).float()\n",
    "    logits = xenc @ W  #(5,27), (27,27) -> (5,27)  \n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True) # predictions # counts and probs form softmax\n",
    "    loss = -probs[torch.arange(5), ys].log().mean()\n",
    "    print(f\"loss:{loss}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set the grads to zero. in pytorch, we set grad to None which is equal to setting it to 0\n",
    "    loss.backward() # calculate grads\n",
    "\n",
    "    # update\n",
    "    W.data += -0.1 * W.grad\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the entire dataset instead of the first word (5 examples)\n",
    "Everything should work the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples =  228146\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chars = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = len(xs)\n",
    "print(\"number of examples = \", num)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad= True) # set requires grad= True to make sure grads for W are stored\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0: 2.4837753772735596\n",
      "loss at iteration 100: 2.483569622039795\n",
      "loss at iteration 200: 2.4833669662475586\n",
      "loss at iteration 300: 2.483166456222534\n",
      "loss at iteration 400: 2.4829680919647217\n",
      "loss at iteration 500: 2.4827725887298584\n",
      "loss at iteration 600: 2.4825797080993652\n",
      "loss at iteration 700: 2.482388734817505\n",
      "loss at iteration 800: 2.4822006225585938\n",
      "loss at iteration 900: 2.4820139408111572\n",
      "loss at iteration 1000: 2.48183012008667\n",
      "loss at iteration 1100: 2.4816482067108154\n",
      "loss at iteration 1200: 2.481468439102173\n",
      "loss at iteration 1300: 2.4812910556793213\n",
      "loss at iteration 1400: 2.4811151027679443\n",
      "loss at iteration 1500: 2.4809417724609375\n",
      "loss at iteration 1600: 2.4807705879211426\n",
      "loss at iteration 1700: 2.4806013107299805\n",
      "loss at iteration 1800: 2.480433464050293\n",
      "loss at iteration 1900: 2.4802677631378174\n",
      "loss at iteration 2000: 2.4801039695739746\n",
      "loss at iteration 2100: 2.4799420833587646\n",
      "loss at iteration 2200: 2.4797821044921875\n",
      "loss at iteration 2300: 2.479623794555664\n",
      "loss at iteration 2400: 2.4794671535491943\n",
      "loss at iteration 2500: 2.4793124198913574\n",
      "loss at iteration 2600: 2.479159116744995\n",
      "loss at iteration 2700: 2.4790077209472656\n",
      "loss at iteration 2800: 2.4788577556610107\n",
      "loss at iteration 2900: 2.4787096977233887\n",
      "loss at iteration 3000: 2.478563070297241\n",
      "loss at iteration 3100: 2.4784178733825684\n",
      "loss at iteration 3200: 2.4782745838165283\n",
      "loss at iteration 3300: 2.478132724761963\n",
      "loss at iteration 3400: 2.477992057800293\n",
      "loss at iteration 3500: 2.4778530597686768\n",
      "loss at iteration 3600: 2.477715492248535\n",
      "loss at iteration 3700: 2.4775795936584473\n",
      "loss at iteration 3800: 2.477444887161255\n",
      "loss at iteration 3900: 2.477311611175537\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i in range(20000):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs,num_classes=27).float()\n",
    "    logits = xenc @ W  #(5,27), (27,27) -> (5,27)\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True) # predictions , equivalent to N\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"loss at iteration {i}: {loss}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set the grads to zero. in pytorch, we set grad to None which is equal to setting it to 0\n",
    "    loss.backward() # calculate grads\n",
    "\n",
    "    # update\n",
    "    W.data += -lr * W.grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The loss stops decreasing after 2.47. This is almost the same loss as we achieved by using counts in the simple bigram model.\n",
    " The loss we achieved in the simple bigram model was based on counts.  Here, the loss is achieved using gradient based optimization.\n",
    "\n",
    " This makes sense because we are not taking any additional information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **AT THE END OF OPTIMIZATION, THE ARRAY W IS BASICALLY THE SAME AS THE MATRIX N IN THE BIGRAM MODEL. THE ONLY DIFFERENCE IS, THAT IN THE SIMPLE MODEL WE POPULATED THAT ARRAY WITH COUNTS, NOW WE ARE INITIALIZING COUNTS RANDOMLY AND OPTIMIZING THEM WITH THE HELP OF THE LOSS TO THE ACTUAL COUNT VALUES.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "pxzfay.\n",
      "a.\n",
      "nn.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) \n",
    "\n",
    "for i in range(5):\n",
    "    result = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # get the probs\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float() # one hot encoding the input. input will be a single character index while sampling\n",
    "        logits = xenc @ W  #(228146,27), (27,27) -> (228146,27)\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        # you have probs, now sample one character at a time from this distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        result.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of sampling is almost the same as the simple bigram model with the same seed as we have reached the same count values as the simple bigram model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
